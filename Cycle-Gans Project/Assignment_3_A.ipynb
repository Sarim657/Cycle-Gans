{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg6hsKMq398n"
      },
      "source": [
        "# ASSIGNMENT 3\n",
        "PART A consists of theoretical questions. Objective answers will receive zero marks. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading and finding about them). Questions that need less explanation can be answered in 95-150 words. Please ensure the answers are well-written and thorough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFQN0GPrnSd3"
      },
      "source": [
        "# PART A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuKQGaqx2a5i"
      },
      "source": [
        "Q 1)What are optimizers in ML. Give some examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usr-qtWP4ZIH"
      },
      "source": [
        "Ans 1.\n",
        "Optimizers in machine learning are algorithms used to adjust the parameters of a model to minimize a loss function. Many optimizers exist with each having its own strengths and weaknesses. Choosing the right optimiser can significantly impact model performance. Some optimisers are better suited for certain types of loss functions. They differ in functioning with respect to different sizes of datasets, some are efficient for large while some are efficient for small ones. Example: Gradient Descent: Stochastic Gradient Descent, Mini-Batch Gradient Descent - these update parameters in the direction that minimises the loss function, Momentum - adds a fraction of previous update to the current update, helping to overcome local minima, RMSProp - Adaptively adjusts the learning rate for each parameter based on its past gradients.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8GPK5j_2n3P"
      },
      "source": [
        "Q 2)Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMS0fLjTAICg"
      },
      "source": [
        "Ans 2.\n",
        "The key differences are as follows -\n",
        "\n",
        "Gradient Descent:\n",
        "\n",
        "1. Updates the parameters of a model using the full dataset in each iteration\n",
        "2. Slow and computationally expensive for large datasets\n",
        "3. Used for small datasets where computational cost is not high\n",
        "\n",
        "Stochastic Gradient Descent:\n",
        "\n",
        "1. Updates the parameters using a single randomly chosen data point in each iteration\n",
        "2. Noisier than Gradient Descent, but much faster and more scalable\n",
        "3. Used for large datasets where computational efficiency is required\n",
        "\n",
        "Mini-Batch Gradient Descent:\n",
        "\n",
        "1. Updates the parameters using a small subset (mini-batch) of the data in each iteration\n",
        "2. It is a compromise between Gradient Descent and Stochastic Gradient Descent. It is a balance between computational cost and convergence speed\n",
        "3. Suitable for most situations, between Gradient Descent and Stochastic Gradient Descent\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AG5Rm7c2sAO"
      },
      "source": [
        "Q 3)Explain about Adam optimizer in detail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDidEe0rIdh3"
      },
      "source": [
        "Ans 3. Adam optimiser stands for adaptive moment estimation optimiser. It maintains a separate adaptive learning rate for each parameter in a dataset. It uses the estimates of the first and second moments of the gradients to update parameters. Some if its advantages are that Adam is fast and efficient for large datasets, it is less sensitive to initial learning rate and works well with various types of loss functions and model architectures. Its disadvantages are that it can be computationally expensive, and sometimes may not be suitable for problems with highly varied gradients.\n",
        "\n",
        "The steps of this optimiser are:\n",
        "1. Initialise learning rate, decay rates for momentum and RMSProp, and initial parameter values\n",
        "2. Run a loop in which for each iteration - we calculate the gradients of the loss function with respect to the parameters, then update the estimates of the first and second moments of the gradients, then calculate the adaptive learning rate for each parameter using estimates, then update the parameters using adaptive learning rate and updated gradient estimates.\n",
        "\n",
        "Adam is used when there are large datasets and complex models and the initial learning rate is unknown/difficult to tune. It is a general purpose optimiser for most machine learning tasks.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0I65BBb2w84"
      },
      "source": [
        "Q 4)Explain the difference between Rmsprop and Adam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO8B03ljLrBJ"
      },
      "source": [
        "Ans 4. The differences are as follows\n",
        "1. RMSProp stands for Root Mean Square Propagation, Adam stands for Adaptive Moment Estimation\n",
        "2. Adam incorporates momentum as a factor and this helps to overcome local minima whereas RMSProp doesn't have momentum\n",
        "3. Adam requires setting 3 hyper parameters i.e., learning rate, beta1, beta2 while initialisation and RMSProp requires setting learning rate only\n",
        "4. The computational cost of Adam is slightly more than RMSProp due to the additional calculations for momentum\n",
        "5. Adam is used as a general optimiser for most machine learning tasks, especially for larger datasets. RMSProp is used while dealing with datasets with sparse gradients and where initial learning rate is unknown\n",
        "6. RMSProp maintains a moving average of squared gradients of each parameter whereas Adam maintains a moving average of squared gradients as well as the gradients for each parameter\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVYyIYOe2w_k"
      },
      "source": [
        "Q 5)What do you think is the best optimizer among all and Why? If you cannot come to conclusive answer, you must list them all and tell in which scenario the one is preferred.Also tell the disadvantages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_YG-ErYNUQJ"
      },
      "source": [
        "Ans 5.\n",
        "\n",
        "There is no single \"best\" optimiser in machine learning, as the choice of optimiser depends on the specific problem and dataset. However, some of the most commonly used and effective optimisers are:\n",
        "\n",
        "1. Stochastic Gradient Descent:\n",
        "\n",
        "Advantages: Simple to implement, computationally efficient, can be used with large datasets.\n",
        "\n",
        "Disadvantages: Can be slow to converge, may not find the global minimum.\n",
        "\n",
        "Preferred scenario: Convex optimization problems, online learning.\n",
        "\n",
        "\n",
        "2. Adam (Adaptive Moment Estimation):\n",
        "\n",
        "Advantages: Fast convergence, robust to noise and outliers, works well with deep learning models.\n",
        "\n",
        "Disadvantages: Can be computationally expensive, may overfit to the training data.\n",
        "\n",
        "Preferred scenario: Deep learning models, image classification, natural language processing.\n",
        "\n",
        "3. RMSprop (Root Mean Square Propagation):\n",
        "\n",
        "Advantages: Fast convergence, robust to noise and outliers, less computationally expensive than Adam.\n",
        "\n",
        "Disadvantages: Can be sensitive to the learning rate, may not work well with deep learning models.\n",
        "\n",
        "Preferred scenario: Recurrent neural networks, time series forecasting.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKxnf5et2xDF"
      },
      "source": [
        "Q 6)What is overfitting and underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6jF-UMYNZLg"
      },
      "source": [
        "Ans 6.\n",
        "1. Overfitting is a situation when a model learns training data in such a way that it can only give accurate predictions to the points that belong to the training dataset. This leads to poor performance in testing data since the model can make predictions only for the points that belong to the training data\n",
        "\n",
        "2. Underfitting is a situation when a model does not learn the training data well enough to give predictions with high accuracy. The model fails to capture the patterns in the given training dataset and leads to poor performance on testing dataset\n",
        "\n",
        "Both of these situations are not ideal and can lead to bad machine learning models which are undesirable\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UCncA8M2xF5"
      },
      "source": [
        "Q 7)Explain the vanishing and exploding gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFkulE4UOfmu"
      },
      "source": [
        "Ans 7.\n",
        "Vanishing gradients and exploding gradients are problems encountered during training neural networks\n",
        "1. Vanishing gradients means that the gradients of the loss function with respect to the earlier layers become very small which leads to slow or no learning in those layers. It is caused when the sigmoid and tanh activation functions saturate for large inputs. It can also happen when there are very small weight values. In this case there is close to zero learning in the earlier layers of the network\n",
        "\n",
        "2. Exploding gradients means that the gradients of the loss function with respect to earlier layers become very large which leads to unstable training and NaN values. It is caused when there are unbounded activation functions. It can also happen when there are large weight values. The training process is very unstable in this case.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNW8Hmd52xIn"
      },
      "source": [
        "Q 8)Explain batch and layer normalization in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9irHx1buPpt_"
      },
      "source": [
        "Ans 8.\n",
        "\n",
        "1. Batch normalisation is a technique used to improve the training of neural networks by normalising inputs to each layer. It is done based on the mean and varience of the batch of inputs. It helps to reduce internal incovariate shift which can slow down training. Its steps are - calculate mean and variance for the batch of inputs, then normalise the inputs by subtracting the mean and dividing by the standard deviation, then scale and shift the normalised inputs using the learned parameters. It is used mostly for deep learning tasks, when the batch size is large\n",
        "\n",
        "2. Layer normalisation is a technique which normalises the inputs to each layer based on the mean and variance of the layer's activations. It is more effective than batch normalisation for recurrent neural networks and in cases where the batch size is small. The steps for layer normalisation are - calculate the mean and variance of the layer's activations, normalise the activations by subtracting the mean and dividing by the standard deviation, scale and shift the normalised activations using learned parameters\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pMnZ6z52xLa"
      },
      "source": [
        "Q 9)What are regularization techniques in machine learning?(200 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V6tBjTOR-rI"
      },
      "source": [
        "Ans 9. Regularisation techniques in machine learning are used to prevent overfitting and improve the ability of a model to generalise. Regularisation techniques neglect large weights, which helps to prevent the model from overfitting with the data. It encourages the model to learn more generalisable patterns rather than the discrete datapoints of the training set. Regularisation can improve model's interpretation capacity leading to a high accuracy rate with the testing dataset. It is used when dealing with small datasets or data with lots of noise. Some common regularisation techniques are\n",
        "- L1 - adds absolute value of weights to loss function\n",
        "- L2 - adds squared value of the weights to loss function\n",
        "- elastic net - combines both the methods of L1 as well as L2 regularisation techniques\n",
        "- dropout techniques - Randomly drop out neurons during training the model\n",
        "\n",
        "Regularisation techniques are chosen on the basis of properties of the data and model, since every type of data has different characteristics and can lead to overfitting in various ways, if we can identify the reason for overfitting, we can easily choose the type of regularisation technique we want to implement for that model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoVZoLpB3B7X"
      },
      "source": [
        "Q 10)What is dropout layer and explain how it prevents overfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIhhHQ6zUPzi"
      },
      "source": [
        "Ans 10.\n",
        "A dropout layer is a regularisation technique used in neural networks. It works by randomly dropping out or setting zero to a certain fraction of neurons during training. This forces the network to learn more representations of the data, so that it can give accurate results even when some neurons are not active. The purpose of this technique is to decrease dependence of the whole model on a small group of neurons.\n",
        "It prevents overfitting because\n",
        "1. It prevents co-adaptation. Overfitting occurs when a neural network learns to rely on specific combination of neurons to make predictions. Hence we drop them to train it better\n",
        "2. Dropout makes the network more uncertain about its predictions, as it is not able to rely on any single neuron too heavily. This can help to prevent the network from making overly confident predictions on unseen data\n",
        "3. It encourages feature reuse instead of relying on other neurons.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMx3OcJ53CBj"
      },
      "source": [
        "Q 11)Explain L1 and L2 regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmxjuWeRVQeQ"
      },
      "source": [
        "Ans 11.\n",
        "\n",
        "1. L1 Regularisation means Lasso regularisation. It adds absolute value of the weights to the loss function. It removes large weights which is a way of normalising the data. It can help the model to learn sparse datasets better. It can be used for feature selection as it tends to drive some weights to zero. It is used when the goal is to obtain a sparse model or for feature selection purposes\n",
        "\n",
        "2. L2 Regularisation means ridge regularisation. It adds the squared value of the weights to the loss function. It removes large weights, similar to L1, but less heavily. Therefore, it has less ability to learn sparse models and generally works on closer datapoints. It can improve stability of the model. It is used when the goal is to prevent overfitting and improve the ability of the model to generalise.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5TGmHuJ3JTh"
      },
      "source": [
        "Q 12)Write about validation accuracy and why we need it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1HO2dMyWH_v"
      },
      "source": [
        "Ans 12.\n",
        "\n",
        "Validation accuracy is the accuracy obtained when the model is trained using the training dataset and tested using the validation dataset. Validation set is a part of the training dataset which is split into training dataset and validation set, generally around 80% proportion to training and 20% proportion to validation set. This validation set is then tested to obtain the accuracy and this is called validation accuracy\n",
        "\n",
        "We need validation accuracy because this indicates if a model is overfitting or not. If a model has high accuracy on the validation set, it means that the model is not overfit, while a low accuracy means that the model is overfitting. It is used to compare different models and select the best performing one. A high validation accuracy also represents the ability of the model to generalise data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCOGr8vl3JXY"
      },
      "source": [
        " Q 13)What do you mean by data augmentation and explain is advantages and disadvantages in detail?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4NPzwQJXa15"
      },
      "source": [
        "Ans 13. Data augmentation is a technique used to artificially increase the size and diversity of a training dataset by applying various transformations to the existing data. This is useful when the original dataset lacks variability. It is done for image, text and audio data.\n",
        "\n",
        "Advantages of data augmentation -\n",
        "1. Reduces overfitting by making the model less reliant on specific details of the training data\n",
        "2. It can make the model resistant to noise\n",
        "3. It improves the overall performance (accuracy, precision, recall, f1 score) of the model\n",
        "4. It increases the ability of a model to generalise the data and learn from the patterns instead of learning the entire dataset.\n",
        "\n",
        "Disadvantages of data augmentation -\n",
        "1. Such a task increases the computational cost of a model and becomes very computationally expensive when using complex data.\n",
        "2. If data augmentation is not used carefully, it can lead to overfitting\n",
        "3. It has a potential for introducing bias into the model\n",
        "\n",
        "Some common data augmentation techniques are -\n",
        "1. Image: cropping, flipping, rotation, scaling\n",
        "2. Text: random insertion, random deletion, synonym replacement\n",
        "3. Audio: noise addition, pitch shifting, increase speed\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeYHJXKh3O1x"
      },
      "source": [
        "Q 14)What is transfer learning.Explain in detail? Mention various pre-trained model present in the community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzQqalVZZgi"
      },
      "source": [
        "Ans 14. Transfer learning is a technique in machine learning where a model trained on one task is used as a starting point for a model on a second task. This is done by transferring the knowledge that the first model has learned to the second model.\n",
        "\n",
        "The general process of transfer learning is as follows\n",
        "\n",
        "A model is trained on a large dataset for a specific task, the pre-trained model is then used as a starting point for a new model that will be trained for a different task, the weights of the pre-trained model are copied to the new model, and finally\n",
        "the new model is then fine-tuned on the new dataset for the new task\n",
        "\n",
        "Its advantages are\n",
        "1. Since the model is already trained on one dataset, it takes overall less time for the training process\n",
        "2. The performance is improved since the model has been trained twice instead of once\n",
        "3. It reduces overfitting as both the types of data which the model is trained on are different and increases the model's ability to generalise the given datapoints and interpret meaningful results out of it\n",
        "\n",
        "The pre-trained models available in the community are\n",
        "\n",
        "ResNet, VGG, Inception for image classification\n",
        "\n",
        "BERT, GPT-3, RoBERTa for Natural Language Processing\n",
        "\n",
        "Wav2Vec2, DeepSpeech, HuBERT for speech recognition\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0x7E-Cg3Vzc"
      },
      "source": [
        "Q 15) Explain the bias- variance tradeoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bToKJl-anFV"
      },
      "source": [
        "Ans 15.\n",
        "The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between two types of error i.e., bias and variance.\n",
        "\n",
        "Bias is the error that is introduced by approximating a real-world problem with a simplified model. For eg, if we use a linear regression model to predict the price of a house, the model will have bias because it cannot capture all of the factors that affect the price of a house.\n",
        "\n",
        "Variance is the error that is introduced by the model's generalisability. For eg, if we train a decision tree model on a small dataset, the model will have high variance because it will be sensitive to the specific data points in the training set.\n",
        "\n",
        "The bias-variance tradeoff states that as the bias is reduced, the variance should increase, and vice versa. This is because as we make our model more complex to reduce bias, we will also make it more likely to overfit to the training data, which will increase variance.\n",
        "\n",
        "The goal of machine learning is to find the optimal balance between bias and variance. This is doneby choosing the right model complexity, the right amount of training data, and implementing the regularisation techniques in the model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUU9eDCa3S2U"
      },
      "source": [
        "Q 16)Assume you have dataset of patients who visited AIIMS(Delhi) between the period of 2018-2020.The datasets include features from CBC reports,IGE,weight,temp,etc.There problems were mainly classified into gastrointestinal problems, heart problems, diabetes and misc.\n",
        "You being a experienced ML engineer, the hospital has approached you to make a model which given these features can predict the problem that the patient is suffering.You can either train a separate neural network for each of the diseases or to train a single neural network\n",
        "with one output neuron for each disease. Which method do you prefer.Justify your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf5vaT6QfO5c"
      },
      "source": [
        "Ans 16.\n",
        "\n",
        "\n",
        "To address this problem, I would prefer to train a single neural network with one output neuron for each disease.\n",
        "\n",
        "This is because if I use multiple neural networks, the error will be different for each and the model will not be as accurate as it would be when trained with single neural network\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRbqSHnH0Cu5"
      },
      "source": [
        "Q 17) Assume your input data X has m observations and n0 features (shape: n0 x m), and the output layer Y has shape (n3 x m). Say that you want to have 2 hidden layers in your model: A1- (n1 x m) and A2- (n2 x m).\n",
        "\n",
        "(a) How many weight and bias matrices will you have? Find the shapes of each & verify if the dimensions of matrix multiplications are accurate.\n",
        "\n",
        "(b) If this is a Binary classification problem, what activation functions will you use for (i) the hidden layers, and (ii) the output layer. Why?\n",
        "\n",
        "(c) Repeat part (b) if this is a Multi-class classification problem.\n",
        "\n",
        "(d) Repeat part (b) if this is a Regression problem.\n",
        "\n",
        "-> Give proper reasoning for each part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWy7rIuOc5o-"
      },
      "source": [
        "Ans 17.\n",
        "\n",
        "a)\n",
        "\n",
        "Weights and Biases between Input Layer and First Hidden Layer:\n",
        "\n",
        "1. Weight matrix W\n",
        "1\n",
        "  (shape:\n",
        "n1×n0)\n",
        "\n",
        "2. Bias vector\n",
        "b\n",
        "1\n",
        "​\n",
        "  (shape:\n",
        "n1×1)\n",
        "\n",
        "\n",
        "Weights and Biases between First Hidden Layer and Second Hidden Layer:\n",
        "\n",
        "1. Weight matrix\n",
        "W\n",
        "2\n",
        "  (shape:\n",
        "n2×n1)\n",
        "2. Bias vector\n",
        "b\n",
        "2\n",
        "  (shape:\n",
        "n2×1)\n",
        "\n",
        "Weights and Biases between Second Hidden Layer and Output Layer:\n",
        "\n",
        "1. Weight matrix\n",
        "W\n",
        "3\n",
        "  (shape:\n",
        "n3×n2)\n",
        "2. Bias vector\n",
        "b\n",
        "3\n",
        "  (shape:\n",
        "n3×1)\n",
        "\n",
        "b)\n",
        "\n",
        "i) For the hidden layers I would use ReLU function because it removes the vanishing gradient problem and accelerates convergence\n",
        "\n",
        "ii) For the output layer I would use sigmoid function because probability lies between 0 and 1 for the binary classification and this function is used for predicting probability\n",
        "\n",
        "c)\n",
        "\n",
        "i) For the hidden layers I would use ReLU function because it removes the vanishing gradient problem and accelerates convergence\n",
        "\n",
        "ii) For the output layer I would use softmax function because it outputs probability distribution over classes\n",
        "\n",
        "d)\n",
        "\n",
        "i) For the hidden layers I would use ReLU function because it removes the vanishing gradient problem and accelerates convergence\n",
        "\n",
        "ii) For the output layer I would use the linear function because for regression a linear activation function should be used to predict continuous values\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
